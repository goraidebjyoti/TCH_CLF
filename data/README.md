# TCH_CLF – Data Documentation

This document describes the **data directory structure, file formats, and data provenance** used by the TCH_CLF pipeline.

Only **core input datasets** are committed to this repository.  
Large external resources and **generated artifacts** are intentionally excluded.

---

## Overview of Data Roles

Data in TCH_CLF serves four distinct roles:

1. **Core input datasets** (committed)
2. **External clinical trial corpus** (not included)
3. **Generated LLM reasoning files** (not included)
4. **Generated evaluation artifacts** (not included)

Each role is isolated in a dedicated subdirectory.

---

## Directory Structure

```
data/
├── README.md
├── dataset/                  # Committed core input datasets
├── clinicaltrials/            # External corpus + generated parsed trials
├── reasoning/                 # Generated synthetic reasoning files
├── 2021/                      # Generated reasoning for TREC 2021
└── 2022/                      # Generated reasoning for TREC 2022
```

---

## 1. Core Input Datasets (`data/dataset/`)

This directory contains **all committed input data** required to run TCH_CLF.

### `synthetic_gold_queries.tsv`
- **Description**: Synthetic patient case descriptions
- **Format**: TSV
- **Columns**:
  ```
  <topic_id>    <query_text>
  ```
- **Usage**:
  - Synthetic reasoning generation
  - Teacher model training

---

### `ct_2021_queries.tsv` / `ct_2022_queries.tsv`
- **Description**: Official TREC Clinical Trials Track queries
- **Format**: TSV
- **Columns**:
  ```
  <query_id>    <query_text>
  ```
- **Usage**:
  - Reasoning generation for TREC 2021/2022
  - Teacher model inference

---

### `train_1196.jsonl`
- **Description**: Triplet dataset used for synthetic reasoning generation
- **Format**: JSON Lines
- **Fields**:
  ```json
  {
    "topic_id": "<id>",
    "trial_id": "<trial_id>",
    "label": "<positive|negative>"
  }
  ```

**Positive trial identification:**  
The positive (relevant) trial for each synthetic query was obtained from resources released by the **IELAB team**, which provide one clinically relevant trial per synthetic patient case. From an initial pool of approximately **20,000 synthetic queries**, **1,200** instances were sampled. After filtering invalid or non-conforming LLM outputs, the final dataset contains **1,196 positive and 1,196 negative patient–trial pairs**.

These training queries and trials are **disjoint from the TREC 2021 and 2022 evaluation topics**, ensuring **no data leakage**.

---

## 2. Clinical Trial Data (`data/clinicaltrials/`)

### External Trial Corpus (Not Included)

- **File**: `corpus.jsonl`
- **Description**: Full ClinicalTrials.gov trial corpus
- **Size**: > 1.8 GB
- **Status**: **External dependency**
- **Important**:
  - This file is **not generated by any code in this repository**
  - It is not included due to size and licensing considerations

Expected format:
```json
{
  "id": "<trial_id>",
  "contents": "<raw trial text>"
}
```

Users must obtain this corpus from ClinicalTrial.gov independently, 
then convert it to the required format as per the codes and place it at:
```
data/clinicaltrials/corpus.jsonl
```

---

### Parsed and Concatenated Trials (Generated)

Generated by:
- `trial_parser.py`
- `concatenation.py`

Expected structure:
```
data/clinicaltrials/
└── parsed/
    ├── extracted_trials.jsonl
    ├── concatenated_trials.jsonl
    ├── extraction_log.txt
    └── concatenation_log.txt
```

These files are **required for training and inference** but are **generated locally** and not committed.

---

## 3. Synthetic Reasoning Files (`data/reasoning/`)

Generated by:
- `gen_reasoning_relevance_syn.py`
- `clean_llm_output.py`

Expected files:
```
data/reasoning/
├── train_1196_deepseek_raw.jsonl
└── train_1196_deepseek_clean.jsonl
```

- Raw files contain unfiltered LLM output
- Clean files contain sanitized JSON with:
  ```json
  {
    "reasoning": "...",
    "relevance": "Relevant | Non-Relevant"
  }
  ```

---

## 4. TREC Evaluation Reasoning (`data/2021/`, `data/2022/`)

Generated by:
- `gen_reasoning_relevance.py`
- `clean_llm_output.py`

Expected structure (example for 2021):
```
data/2021/
└── reasoning/
    ├── WholeQ_RETRIEVAL_T2021_deepseek_raw.jsonl
    ├── WholeQ_RETRIEVAL_T2021_deepseek_clean.jsonl
    ├── WholeQ_RM3_RETRIEVAL_T2021_deepseek_raw.jsonl
    └── WholeQ_RM3_RETRIEVAL_T2021_deepseek_clean.jsonl
```

The same structure applies to `data/2022/`.

---

## Data Generation Notes

- Generated directories may need to be **created manually** before execution
- LLM outputs are **non-deterministic** and may vary slightly across runs
- Sanitization scripts must be run before training or inference

---

## Questions

If you encounter missing-path or missing-file errors, verify that:
1. The directory exists
2. The required generation step has been run
3. The external corpus has been placed correctly
