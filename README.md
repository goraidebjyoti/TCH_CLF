# TCH_CLF  
**Reasoning-Augmented Teacher Classifier for Clinical Trial Relevance**

---

## Overview

TCH_CLF is a **reasoning-augmented neural classifier** for clinical trial relevance modeling.  
The framework combines:

- **Natural-language eligibility reasoning generated by a large language model (LLM)**
- **Eligibility-focused subsets of clinical trial text**
- A **Clinical-Longformer encoder** trained with a hybrid classification and ranking objective

TCH_CLF is designed as a **strong teacher model / neural baseline** for re-ranking clinical trials under standard **TREC-style evaluation** settings.

This repository contains **all preprocessing, training, and inference code**, while **large external resources and generated artifacts are intentionally excluded**.

---

## Key Characteristics

- Explicit use of **LLM-generated eligibility reasoning**
- Operates on **parsed and concatenated trial fields** (not full trial documents)
- Built on **Clinical-Longformer** for long-context modeling
- Trained with a **hybrid objective**:
  - Pointwise relevance classification (binary cross-entropy)
  - Pairwise ranking loss
- Supports **multi-GPU training**, AMP, and gradient checkpointing

---

## High-Level Pipeline

```
Raw ClinicalTrials.gov Corpus (external, >1.8 GB)
        ↓
Trial Parsing & Field Extraction
        ↓
Eligibility-Focused Trial Concatenation
        ↓
LLM-Based Reasoning + Relevance Generation
        ↓
LLM Output Sanitization
        ↓
Teacher Model Training (Clinical-Longformer)
        ↓
Inference & Re-ranking (TREC format)
```

---

## Verified Repository Structure

The repository is organized as follows:

```
TCH_CLF/
├── README.md
├── data/
│   ├── README.md
│   ├── dataset/                  # Committed core inputs (queries, triplets)
│   ├── clinicaltrials/            # External corpus + generated parsed trials
│   ├── reasoning/                 # Generated synthetic reasoning files
│   ├── 2021/                      # Generated reasoning files for TREC 2021
│   └── 2022/                      # Generated reasoning files for TREC 2022
├── models_new/                    # Generated teacher model checkpoints
├── output/                        # Generated inference outputs
├── train_tch_clf.py               # Teacher model training
├── inference_tch_clf.py           # Inference and re-ranking
├── trial_parser.py                # Trial field extraction
├── concatenation.py               # Eligibility-focused trial concatenation
├── gen_reasoning_relevance_syn.py # LLM reasoning generation (synthetic data)
├── gen_reasoning_relevance.py     # LLM reasoning generation (TREC data)
└── clean_llm_output.py            # LLM output sanitization
```

**Note on generated files:**  
Several directories and files shown above (e.g., parsed trials, reasoning files, model checkpoints, inference outputs) are **created during execution** and will **not be present in a fresh clone** of the repository.

---

## Data Organization (Summary)

The `data/` directory is organized by **data role**:

- **`data/dataset/`**  
  Core, committed input datasets:
  - Synthetic patient queries
  - Query files for TREC 2021/2022
  - Triplet training file (`train_1196.jsonl`)

- **`data/clinicaltrials/`**  
  ClinicalTrials.gov data and its processed derivatives:
  - `corpus.jsonl` (external, not included)
  - `parsed/extracted_trials.jsonl` (generated)
  - `parsed/concatenated_trials.jsonl` (generated)

- **`data/reasoning/`**  
  Generated reasoning files for **synthetic training data**

- **`data/2021/` and `data/2022/`**  
  Generated reasoning files for **TREC 2021 and 2022** retrieval experiments

Full details are provided in **`data/README.md`**.

---

## Trial Text Processing

Instead of using full clinical trial documents, TCH_CLF operates on **eligibility-relevant subsets** extracted from raw trial records.

- **`trial_parser.py`**
  - Extracts structured fields:
    - study title
    - brief summary
    - conditions
    - gender eligibility
    - age limits
    - eligibility criteria

- **`concatenation.py`**
  - Concatenates extracted fields into a compact textual representation
  - Excludes interventions and non-essential metadata

This design reduces noise while preserving information critical for eligibility reasoning.

---

## Reasoning and Relevance Generation

LLM-based reasoning is generated using **DeepSeek-R1-Distill-Qwen-32B**.

Two generation modes are supported:

1. **Synthetic training data**
   - Generates reasoning and relevance labels for curated query–trial triplets

2. **Real retrieval data (TREC 2021 / 2022)**
   - Generates reasoning for BM25 and BM25+RM3 retrieved trials

Raw LLM outputs are sanitized using a shared cleaning script before downstream use.

---

## Note on LLM Choice and Reproducibility

**Important:** All reasoning and relevance annotations in this repository were generated using  
**DeepSeek-R1-Distill-Qwen-32B**.

The teacher classifier is trained **directly on these LLM-generated signals**.  
Changing the LLM (or its prompting strategy) will alter the distribution, structure, and style of the generated reasoning and relevance labels, and therefore **invalidate direct comparison with the results reported in the paper**.

Additionally, different LLMs employ **different chat templates, tokenization schemes, and instruction-following conventions** (e.g., system/user roles, special tokens, or reasoning delimiters). As a result, **code-level modifications may be required** when substituting the LLM, including but not limited to:
- Prompt formatting and chat template construction
- Input tokenization and special token handling
- Output parsing and JSON extraction logic

Users may experiment with alternative LLMs for exploratory or extension purposes; however, to **faithfully reproduce the reported results**, the same LLM, prompt templates, and output-processing logic should be used.

---

## Teacher Model Training

- Encoder: `yikuan8/Clinical-Longformer`
- Input format:
  ```
  [Query] + [Concatenated Trial Text] + [LLM Reasoning]
  ```
- Training objective:
  ```
  L = BCE + α · PairwiseRankingLoss
  ```
- Key hyperparameters:
  - Epochs: 10
  - Early stopping patience: 5
  - α ∈ {0.1, 0.2, 0.3}
  - **α = 0.2 selected as best**
- Training optimizations:
  - Mixed precision (AMP)
  - Gradient checkpointing
  - Multi-GPU training

---

## Inference

`inference_tch_clf.py` applies the trained teacher model to:
- TREC Clinical Trials Track 2021 and 2022
- WholeQ and WholeQ+RM3 retrieval runs

Outputs are written in **standard TREC run format** and sorted numerically by topic ID.

---

## Hard Negative Construction

Negative training examples are selected using an **eligibility-based hard-negative strategy** inspired by prior work.  
For reference, see the NEUREQ framework:

https://github.com/goraidebjyoti/NEUREQ

This repository does **not** depend on NEUREQ code.

---

## Hardware & Environment

- GPU: NVIDIA L40 / A100 / H100 recommended
- GPU memory: ≥ 24 GB
- Disk space: ≥ 120 GB
- Python ≥ 3.9
- PyTorch ≥ 2.0
- CUDA ≥ 11.8

---

## Data Availability

The **ClinicalTrials.gov corpus is external** and **not generated by any code in this repository**.  
It is not included due to its large size (**>1.8 GB**).

Generated artifacts such as:
- Parsed trial files
- LLM reasoning outputs
- Model checkpoints
- Inference runs

are created during execution and are **not included by default**.

---

## Disclaimer

This repository is intended for **research and academic use only**.  
It is **not a clinical decision support system**.
